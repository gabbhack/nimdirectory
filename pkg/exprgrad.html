<!DOCTYPE html>
<html lang="en">
<head>
  <title>Nim Package Directory</title>
  <link rel="shortcut icon" href="data:image/x-icon;base64,AAABAAEAEBAAAAEAIABoBAAAFgAAACgAAAAQAAAAIAAAAAEAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AAAAAAUAAAAF////AP///wD///8A////AP///wD///8A////AP///wD///8A////AAAAAAIAAABbAAAAlQAAAKIAAACbAAAAmwAAAKIAAACVAAAAWwAAAAL///8A////AP///wD///8A////AAAAABQAAADAAAAAYwAAAA3///8A////AP///wD///8AAAAADQAAAGMAAADAAAAAFP///wD///8A////AP///wAAAACdAAAAOv///wD///8A////AP///wD///8A////AP///wD///8AAAAAOgAAAJ3///8A////AP///wAAAAAnAAAAcP///wAAAAAoAAAASv///wD///8A////AP///wAAAABKAAAAKP///wAAAABwAAAAJ////wD///8AAAAAgQAAABwAAACIAAAAkAAAAJMAAACtAAAAFQAAABUAAACtAAAAkwAAAJAAAACIAAAAHAAAAIH///8A////AAAAAKQAAACrAAAAaP///wD///8AAAAARQAAANIAAADSAAAARf///wD///8AAAAAaAAAAKsAAACk////AAAAADMAAACcAAAAnQAAABj///8A////AP///wAAAAAYAAAAGP///wD///8A////AAAAABgAAACdAAAAnAAAADMAAAB1AAAAwwAAAP8AAADpAAAAsQAAAE4AAAAb////AP///wAAAAAbAAAATgAAALEAAADpAAAA/wAAAMMAAAB1AAAAtwAAAOkAAAD/AAAA/wAAAP8AAADvAAAA3gAAAN4AAADeAAAA3gAAAO8AAAD/AAAA/wAAAP8AAADpAAAAtwAAAGUAAAA/AAAA3wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAADfAAAAPwAAAGX///8A////AAAAAEgAAADtAAAAvwAAAL0AAADGAAAA7wAAAO8AAADGAAAAvQAAAL8AAADtAAAASP///wD///8A////AP///wD///8AAAAAO////wD///8A////AAAAAIcAAACH////AP///wD///8AAAAAO////wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A//8AAP//AAD4HwAA7/cAAN/7AAD//wAAoYUAAJ55AACf+QAAh+EAAAAAAADAAwAA4AcAAP5/AAD//wAA//8AAA=="/>
  <meta charset="utf-8">
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="alternate" type="application/rss+xml" title="New and updated Nim packages" href="https://nimble.directory/packages.xml">
  <link rel="stylesheet" href="/css/bootstrap.min.css">
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/css/highlite.css">
  <script src="https://cdn.jsdelivr.net/npm/fuse.js@6.6.2"></script>
  <script type="text/javascript" src="/js/app.js"></script>
</head>
<body>
<nav class="navbar navbar-expand-lg fixed-top py-3">
  <div class="container">
    <a href="/" class="logo fw-500 display-1 text-black text-decoration-none navbar-brand"></a>
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse">
      <ul class="navbar-nav ms-auto mb-2 mb-lg-0">
        <li class="nav-item mx-2">
          <a href="https://nim-lang.org/" class="nav-link">What's Nim?</a>
        </li>
        <li class="nav-item mx-2">
          <a href="/about.html" class="nav-link">What's Nimble?</a>
        </li>
        <li class="nav-item mx-2">
          <a href="https://github.com/nim-lang/packages/" class="nav-link">Publish your package</a>
        </li>
        <!-- TODO: replace with something different -->
        <li class="nav-item ms-3">
          <a class="nav-link" href="https://www.youtube.com/channel/UCDAYn_VFt0VisL5-1a5Dk7Q/">
            <svg viewBox="0 0 24 24" width="18" height="18" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round" class="css-i6dzq1"><path d="M22.54 6.42a2.78 2.78 0 0 0-1.94-2C18.88 4 12 4 12 4s-6.88 0-8.6.46a2.78 2.78 0 0 0-1.94 2A29 29 0 0 0 1 11.75a29 29 0 0 0 .46 5.33A2.78 2.78 0 0 0 3.4 19c1.72.46 8.6.46 8.6.46s6.88 0 8.6-.46a2.78 2.78 0 0 0 1.94-2 29 29 0 0 0 .46-5.25 29 29 0 0 0-.46-5.33z"></path><polygon points="9.75 15.02 15.5 11.75 9.75 8.48 9.75 15.02"></polygon></svg>
          </a>
        </li>
        <li class="nav-item ms-3">
          <a class="nav-link theme-switcher-btn" id="darkmode" onclick="toggle_dark_mode()" href="#">
            <span id="light-mode-icon">
            <svg viewBox="0 0 24 24" width="18" height="18" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round" class="css-i6dzq1"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></svg>
            </span>
            <span id="dark-mode-icon">
            <svg viewBox="0 0 24 24" width="18" height="18" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round" class="css-i6dzq1"><circle cx="12" cy="12" r="5"></circle><line x1="12" y1="1" x2="12" y2="3"></line><line x1="12" y1="21" x2="12" y2="23"></line><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line><line x1="1" y1="12" x2="3" y2="12"></line><line x1="21" y1="12" x2="23" y2="12"></line><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line></svg>
            </span>
          </a>
        </li>
      </ul>
    </div>
  </div>
</nav>

<div class="content">
  <div class="container">
    <div class="container pt-10">
        <h3 class="mb-3 fw-bold display-6 pt-4">exprgrad</h3>
        <p class="tags">
            
                <span class="tag">
                    <button class="btn-tag pkg-btn-tag">machine-learning</button></a>
                </span>
            
                <span class="tag">
                    <button class="btn-tag pkg-btn-tag">nn</button></a>
                </span>
            
                <span class="tag">
                    <button class="btn-tag pkg-btn-tag">neural</button></a>
                </span>
            
                <span class="tag">
                    <button class="btn-tag pkg-btn-tag">tensor</button></a>
                </span>
            
                <span class="tag">
                    <button class="btn-tag pkg-btn-tag">array</button></a>
                </span>
            
                <span class="tag">
                    <button class="btn-tag pkg-btn-tag">matrix</button></a>
                </span>
            
                <span class="tag">
                    <button class="btn-tag pkg-btn-tag">ndarray</button></a>
                </span>
            
                <span class="tag">
                    <button class="btn-tag pkg-btn-tag">dsl</button></a>
                </span>
            
                <span class="tag">
                    <button class="btn-tag pkg-btn-tag">automatic-differentiation</button></a>
                </span>
            
        </p>
        <p class="pkg-desc">some("An experimental deep learning framework")</p>
        <a title="Copy" onclick="document.querySelector('#cmd').select();document.execCommand('copy');"
            alt="Copy on clipboard">
            <i class="fa fa-copy"></i>
        </a>
        <input id="cmd" onclick="this.select();" value="nimble install exprgrad" readonly="">
        <br>
        <small style="font-size: 0.8rem;">Need help? Read <a
                href="https://github.com/nim-lang/nimble#creating-packages">Nimble</a></small>
    </div>

    <div class="container row pt-4" id="pkg-content">
        <div class="col-8 box rounded p-3" id="readme-section">
            
                <document><h1>Exprgrad</h1>
<p>Exprgrad is an experimental deep learning framework for Nim based on a differentiable array programming language.
Exprgrad makes creating and training neural networks easy:</p>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="kwd">import</span> <span class="ide">std</span><span class="op">/</span><span class="ide">random</span></div></div><div class="line"><div class="line-content"><span class="kwd">import</span> <span class="ide">exprgrad</span>, <span class="ide">exprgrad</span><span class="op">/</span><span class="ide">layers</span><span class="op">/</span>[<span class="ide">base</span>, <span class="ide">dnn</span>]</div></div><div class="line"><div class="line-content"><span class="ide">randomize</span>(<span class="num">10</span>)</div></div><div class="line"><div class="line-content"></div></div><div class="line"><div class="line-content"><span class="kwd">let</span></div></div><div class="line"><div class="line-content">  <span class="ide">net</span> <span class="op">=</span> <span class="ide">input</span>(<span class="str">&quot;x&quot;</span>)</div></div><div class="line"><div class="line-content">    <span class="op">.</span><span class="ide">dense</span>(<span class="num">2</span>, <span class="num">4</span>)<span class="op">.</span><span class="ide">leakyRelu</span>()  <span class="com"># 1st Layer</span></div></div><div class="line"><div class="line-content">    <span class="op">.</span><span class="ide">dense</span>(<span class="num">4</span>, <span class="num">1</span>)<span class="op">.</span><span class="ide">sigmoid</span>()    <span class="com"># 2nd Layer</span></div></div><div class="line"><div class="line-content">    <span class="op">.</span><span class="ide">target</span>(<span class="str">&quot;predict&quot;</span>)</div></div><div class="line"><div class="line-content">    <span class="op">.</span><span class="ide">mse</span>(<span class="ide">input</span>(<span class="str">&quot;y&quot;</span>))          <span class="com"># Loss</span></div></div><div class="line"><div class="line-content">    <span class="op">.</span><span class="ide">target</span>(<span class="str">&quot;loss&quot;</span>)</div></div><div class="line"><div class="line-content">    <span class="op">.</span><span class="ide">backprop</span>(<span class="ide">gradientDescent</span><span class="op">.</span><span class="ide">makeOpt</span>(<span class="ide">rate</span><span class="op">=</span><span class="num">0.1</span>)) <span class="com"># Train</span></div></div><div class="line"><div class="line-content">    <span class="op">.</span><span class="ide">target</span>(<span class="str">&quot;train&quot;</span>)</div></div><div class="line"><div class="line-content">  <span class="ide">model</span> <span class="op">=</span> <span class="ide">compile</span>[<span class="ide">float32</span>](<span class="ide">net</span>)</div></div><div class="line"><div class="line-content"></div></div><div class="line"><div class="line-content"><span class="kwd">let</span></div></div><div class="line"><div class="line-content">  <span class="ide">trainX</span> <span class="op">=</span> <span class="ide">Tensor</span><span class="op">.</span><span class="ide">new</span>([<span class="num">4</span>, <span class="num">2</span>], <span class="op">@</span>[<span class="ide">float32</span> <span class="num">0</span>, <span class="num">0</span>, <span class="num">0</span>, <span class="num">1</span>, <span class="num">1</span>, <span class="num">0</span>, <span class="num">1</span>, <span class="num">1</span>])</div></div><div class="line"><div class="line-content">  <span class="ide">trainY</span> <span class="op">=</span> <span class="ide">Tensor</span><span class="op">.</span><span class="ide">new</span>([<span class="num">4</span>, <span class="num">1</span>], <span class="op">@</span>[<span class="ide">float32</span> <span class="num">0</span>, <span class="num">1</span>, <span class="num">1</span>, <span class="num">0</span>])</div></div><div class="line"><div class="line-content"></div></div><div class="line"><div class="line-content"><span class="kwd">for</span> <span class="ide">epoch</span> <span class="kwd">in</span> <span class="num">0.</span><span class="op">.&lt;</span><span class="num">5000</span>:</div></div><div class="line"><div class="line-content">  <span class="ide">model</span><span class="op">.</span><span class="ide">apply</span>(<span class="str">&quot;train&quot;</span>, {<span class="str">&quot;x&quot;</span>: <span class="ide">trainX</span>, <span class="str">&quot;y&quot;</span>: <span class="ide">trainY</span>})</div></div><div class="line"><div class="line-content"></div></div><div class="line"><div class="line-content"><span class="ide">echo</span> <span class="ide">model</span><span class="op">.</span><span class="ide">call</span>(<span class="str">&quot;predict&quot;</span>, {<span class="str">&quot;x&quot;</span>: <span class="ide">trainX</span>})</div></div>  </code></pre>
<p>Because exprgrad is based on a custom differentiable programming language, we do not need to rely on its built in layers.
Instead we can also specify the same model in terms of scalar operations on tensors.</p>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="com"># Layer 1</span></div></div><div class="line"><div class="line-content"><span class="ide">hidden</span><span class="op">*</span>[<span class="ide">y</span>, <span class="ide">x</span>] <span class="op">++=</span> <span class="ide">input</span>(<span class="str">&quot;x&quot;</span>)[<span class="ide">y</span>, <span class="ide">it</span>] <span class="op">*</span> <span class="ide">param</span>([<span class="num">2</span>, <span class="num">4</span>])[<span class="ide">it</span>, <span class="ide">x</span>] <span class="op">|</span> (<span class="ide">y</span>, <span class="ide">x</span>, <span class="ide">it</span>)</div></div><div class="line"><div class="line-content"><span class="ide">hidden</span>[<span class="ide">y</span>, <span class="ide">x</span>] <span class="op">++=</span> <span class="ide">param</span>([<span class="num">4</span>])[<span class="ide">x</span>] <span class="op">|</span> (<span class="ide">y</span>, <span class="ide">x</span>)</div></div><div class="line"><div class="line-content"><span class="ide">hiddenRelu</span><span class="op">*</span>{<span class="ide">it</span>} <span class="op">++=</span> <span class="ide">select</span>(<span class="ide">hidden</span>{<span class="ide">it</span>} <span class="op">&lt;=</span> <span class="num">0.0</span>, <span class="num">0.1</span> <span class="op">*</span> <span class="ide">hidden</span>{<span class="ide">it</span>}, <span class="ide">hidden</span>{<span class="ide">it</span>}) <span class="op">|</span> <span class="ide">it</span></div></div><div class="line"><div class="line-content"><span class="com"># Layer 2</span></div></div><div class="line"><div class="line-content"><span class="ide">output</span><span class="op">*</span>[<span class="ide">y</span>, <span class="ide">x</span>] <span class="op">++=</span> <span class="ide">hiddenRelu</span>[<span class="ide">y</span>, <span class="ide">it</span>] <span class="op">*</span> <span class="ide">param</span>([<span class="num">4</span>, <span class="num">1</span>])[<span class="ide">it</span>, <span class="ide">x</span>] <span class="op">|</span> (<span class="ide">y</span>, <span class="ide">x</span>, <span class="ide">it</span>)</div></div><div class="line"><div class="line-content"><span class="ide">output</span>[<span class="ide">y</span>, <span class="ide">x</span>] <span class="op">++=</span> <span class="ide">param</span>([<span class="num">1</span>])[<span class="ide">x</span>] <span class="op">|</span> (<span class="ide">y</span>, <span class="ide">x</span>)</div></div><div class="line"><div class="line-content"><span class="ide">outputSigmoid</span><span class="op">*</span>{<span class="ide">it</span>} <span class="op">++=</span> <span class="num">1.0</span> <span class="op">/</span> (<span class="num">1.0</span> <span class="op">+</span> <span class="ide">exp</span>(<span class="op">-</span><span class="ide">output</span>{<span class="ide">it</span>})) <span class="op">|</span> <span class="ide">it</span></div></div><div class="line"><div class="line-content"><span class="kwd">let</span> <span class="ide">pred</span> <span class="op">=</span> <span class="ide">outputSigmoid</span><span class="op">.</span><span class="ide">target</span>(<span class="str">&quot;predict&quot;</span>)</div></div><div class="line"><div class="line-content"><span class="ide">loss</span><span class="op">*</span>[<span class="num">0</span>] <span class="op">++=</span> <span class="ide">sq</span>(<span class="ide">pred</span>{<span class="ide">it</span>} <span class="op">-</span> <span class="ide">input</span>(<span class="str">&quot;y&quot;</span>){<span class="ide">it</span>}) <span class="op">|</span> <span class="ide">it</span> <span class="com"># Loss</span></div></div><div class="line"><div class="line-content"></div></div><div class="line"><div class="line-content"><span class="kwd">proc</span> <span class="ide">optim</span>(<span class="ide">param</span>: <span class="kwd">var</span> <span class="ide">Fun</span>, <span class="ide">grad</span>: <span class="ide">Fun</span>) <span class="op">=</span></div></div><div class="line"><div class="line-content">  <span class="ide">param</span>{<span class="ide">it</span>} <span class="op">++=</span> <span class="op">-</span><span class="num">0.1</span> <span class="op">*</span> <span class="ide">grad</span>{<span class="ide">it</span>} <span class="op">|</span> <span class="ide">it</span></div></div><div class="line"><div class="line-content"></div></div><div class="line"><div class="line-content"><span class="kwd">let</span> <span class="ide">net</span> <span class="op">=</span> <span class="ide">loss</span><span class="op">.</span><span class="ide">target</span>(<span class="str">&quot;loss&quot;</span>)<span class="op">.</span><span class="ide">backprop</span>(<span class="ide">optim</span>)<span class="op">.</span><span class="ide">target</span>(<span class="str">&quot;train&quot;</span>) <span class="com"># Train</span></div></div><div class="line"><div class="line-content"></div></div><div class="line"><div class="line-content"><span class="kwd">let</span> <span class="ide">model</span> <span class="op">=</span> <span class="ide">compile</span>[<span class="ide">float32</span>](<span class="ide">net</span>)</div></div>  </code></pre>
<p>Since exprgrad&apos;s compiler is able to derive any program written in its domain specific language, we do not need to specify a backwards pass.
This allows you to iterate on custom layers quickly, while avoiding errors in the gradient computation.
The model is optimized and compiled using a JIT compiler, enabling fast execution times.
All layers provided by exprgrad are also implemented in the same way, allowing you to customize them easily.</p>
<h2>Installation</h2>
<p><strong>Warning:</strong> Exprgrad is still very early in its development.
Although all shown examples already work, bugs are expected and important features for training large models (especially Multithreading and GPU support) might still be missing.
Please report any issues you might encounter.</p>
<h3>Ubuntu</h3>
<pre>  <code class="language-bash">$ sudo apt install llvm-13-dev
$ nimble install exprgrad
</code></pre>
<p><strong>Note:</strong> Your version of Ubuntu may not have the <code>llvm-13-dev</code> package in its repositories.
Follow the instructions at <a href="https://apt.llvm.org/">apt.llvm.org</a> to install the required repository.</p>
<h3>Fedora 36</h3>
<pre>  <code class="language-bash">$ sudo dnf install llvm13-devel
$ nimble install exprgrad
</code></pre>
<h3>Fedora 35</h3>
<pre>  <code class="language-bash">$ sudo dnf install llvm-devel
$ nimble install exprgrad
</code></pre>
<h2>Documentation</h2>
<h3>Language</h3>
<p>Exprgrad&apos;s custom differentiable array programming language is used to specify all layers.
It is a custom language which differs greatly from Nim both in syntax and semantics.
Kernels/layers written in exprgrad&apos;s language are embedded in Nim programs and created using the <code>++=</code> macro.</p>
<p>The language does not have functions, procedures or structured control flow.
Instead each program is a single expression inside a series of implicitly specified nested loops.
A simple program which multiplies two matrices looks like this:</p>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="kwd">proc</span> <span class="ide">matmul</span>(<span class="ide">a</span>, <span class="ide">b</span>: <span class="ide">Fun</span>): <span class="ide">Fun</span> <span class="op">=</span></div></div><div class="line"><div class="line-content">  <span class="ide">result</span>[<span class="ide">y</span>, <span class="ide">x</span>] <span class="op">++=</span> <span class="ide">a</span>[<span class="ide">y</span>, <span class="ide">it</span>] <span class="op">*</span> <span class="ide">b</span>[<span class="ide">it</span>, <span class="ide">x</span>] <span class="op">|</span> (<span class="ide">y</span>, <span class="ide">x</span>, <span class="ide">it</span>)</div></div>  </code></pre>
<p>The same program in Nim would look like this:</p>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="kwd">proc</span> `<span class="op">*</span>`<span class="op">*</span>[<span class="ide">T</span>](<span class="ide">a</span>, <span class="ide">b</span>: <span class="ide">Tensor</span>[<span class="ide">T</span>]): <span class="ide">Tensor</span>[<span class="ide">T</span>] <span class="op">=</span></div></div><div class="line"><div class="line-content">  <span class="ide">result</span> <span class="op">=</span> <span class="ide">Tensor</span>[<span class="ide">T</span>]<span class="op">.</span><span class="ide">new</span>([<span class="ide">a</span><span class="op">.</span><span class="ide">shape</span>[<span class="num">0</span>], <span class="ide">b</span><span class="op">.</span><span class="ide">shape</span>[<span class="num">1</span>]])</div></div><div class="line"><div class="line-content">  <span class="kwd">for</span> <span class="ide">y</span> <span class="kwd">in</span> <span class="num">0.</span><span class="op">.&lt;</span><span class="ide">result</span><span class="op">.</span><span class="ide">shape</span>[<span class="num">0</span>]:</div></div><div class="line"><div class="line-content">    <span class="kwd">for</span> <span class="ide">it</span> <span class="kwd">in</span> <span class="num">0.</span><span class="op">.&lt;</span><span class="ide">a</span><span class="op">.</span><span class="ide">shape</span>[<span class="num">1</span>]:</div></div><div class="line"><div class="line-content">      <span class="kwd">for</span> <span class="ide">x</span> <span class="kwd">in</span> <span class="num">0.</span><span class="op">.&lt;</span><span class="ide">result</span><span class="op">.</span><span class="ide">shape</span>[<span class="num">1</span>]:</div></div><div class="line"><div class="line-content">        <span class="ide">result</span>[<span class="ide">y</span>, <span class="ide">x</span>] <span class="op">+=</span> <span class="ide">a</span>[<span class="ide">y</span>, <span class="ide">it</span>] <span class="op">*</span> <span class="ide">b</span>[<span class="ide">it</span>, <span class="ide">x</span>]</div></div>  </code></pre>
<p>As you can see, the program in exprgrad&apos;s domain-specific language is basically equivalent to the last line of the Nim program.
The shape of the output tensor and the iteration ranges of all loops are inferred automatically.</p>
<p>In contrast to Nim, exprgrad&apos;s type system is very simple as it includes only four types.</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td>  <code>Scalar</code></td>
<td>Floating point value. Is differentiable.</td>
</tr>
<tr>
<td>  <code>Index</code></td>
<td>Integer value. Used to index into tensors.</td>
</tr>
<tr>
<td>  <code>Boolean</code></td>
<td>Boolean value. Only used in <code>select</code> instructions.</td>
</tr>
<tr>
<td>  <code>Array[T]</code></td>
<td>Fixed size array with items of type T.</td>
</tr></tbody></table>
<p>Tensors may be accessed using the <code>[]</code> and <code>{}</code> operators.
While <code>[]</code> allows you index into each dimension, <code>{}</code> gives you direct access to the data of the tensor.
Because <code>{}</code> does not allow exprgrad to infer tensor shapes in all cases, <code>[]</code> should always be preferred over <code>{}</code>.</p>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="kwd">proc</span> <span class="ide">identity</span><span class="op">*</span>(<span class="ide">a</span>: <span class="ide">Fun</span>): <span class="ide">Fun</span> <span class="op">=</span></div></div><div class="line"><div class="line-content">  <span class="ide">result</span>{<span class="ide">it</span>} <span class="op">++=</span> <span class="ide">a</span>{<span class="ide">it</span>} <span class="op">|</span> <span class="ide">it</span></div></div>  </code></pre>
<p>Literals for each type are available.
Note that exprgrad does not have automatic type conversions.
<code>Scalar</code> literals therefore must include a point (<code>2.0</code> instead of <code>2</code>) to differentiate them from <code>Index</code> literals.</p>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="kwd">proc</span> <span class="ide">double</span><span class="op">*</span>(<span class="ide">a</span>: <span class="ide">Fun</span>): <span class="ide">Fun</span> <span class="op">=</span></div></div><div class="line"><div class="line-content">  <span class="ide">result</span>{<span class="ide">it</span>} <span class="op">++=</span> <span class="ide">a</span>{<span class="ide">it</span>} <span class="op">*</span> <span class="num">2.0</span> <span class="op">|</span> <span class="ide">it</span></div></div>  </code></pre>
<p>Variables from Nim may be included as static values.
Only variables of type <code>int</code>, <code>float64</code> and <code>bool</code> can be included.</p>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="kwd">proc</span> `<span class="op">*</span>`<span class="op">*</span>(<span class="ide">a</span>: <span class="ide">Fun</span>, <span class="ide">factor</span>: <span class="ide">float64</span>): <span class="ide">Fun</span> <span class="op">=</span></div></div><div class="line"><div class="line-content">  <span class="ide">result</span>{<span class="ide">it</span>} <span class="op">++=</span> <span class="ide">a</span>{<span class="ide">it</span>} <span class="op">*</span> <span class="ide">factor</span> <span class="op">|</span> <span class="ide">it</span></div></div>  </code></pre>
<p>Conditionals can be emulated using the <code>select</code> instruction.
There is no guarantee that both branches are executed.</p>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="kwd">proc</span> <span class="ide">relu</span><span class="op">*</span>(<span class="ide">inp</span>: <span class="ide">Fun</span>): <span class="ide">Fun</span> <span class="op">=</span></div></div><div class="line"><div class="line-content">  <span class="ide">result</span>{<span class="ide">it</span>} <span class="op">++=</span> <span class="ide">select</span>(<span class="ide">inp</span>{<span class="ide">it</span>} <span class="op">&gt;=</span> <span class="num">0.0</span>, <span class="ide">inp</span>{<span class="ide">it</span>}, <span class="num">0.0</span>) <span class="op">|</span> <span class="ide">it</span></div></div>  </code></pre>
<p>An expression may contain multiple statements separated using <code>;</code>.
This allows you to define variables using the <code>let</code> statement and use them later on.</p>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="kwd">proc</span> <span class="ide">tanh</span><span class="op">*</span>(<span class="ide">inp</span>: <span class="ide">Fun</span>): <span class="ide">Fun</span> <span class="op">=</span></div></div><div class="line"><div class="line-content">  <span class="ide">result</span>{<span class="ide">it</span>} <span class="op">++=</span> (</div></div><div class="line"><div class="line-content">    <span class="kwd">let</span> <span class="ide">a</span> <span class="op">=</span> <span class="ide">exp</span>(<span class="ide">inp</span>{<span class="ide">it</span>});</div></div><div class="line"><div class="line-content">    <span class="kwd">let</span> <span class="ide">b</span> <span class="op">=</span> <span class="ide">exp</span>(<span class="op">-</span><span class="ide">inp</span>{<span class="ide">it</span>});</div></div><div class="line"><div class="line-content">    (<span class="ide">a</span> <span class="op">-</span> <span class="ide">b</span>) <span class="op">/</span> (<span class="ide">a</span> <span class="op">+</span> <span class="ide">b</span>)</div></div><div class="line"><div class="line-content">  ) <span class="op">|</span> <span class="ide">it</span></div></div>  </code></pre>
<p>If exprgrad is not able to infer the shape of a tensor, it can be explicitly specified using <code>withShape</code> or <code>copyShape</code>.
The argument to the <code>withShape</code> macro must be of the form <code>[dim0, dim1, dim2, ...]</code> where each dimension is a valid expression in exprgrad&apos;s language.</p>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="kwd">proc</span> <span class="ide">upsample2</span><span class="op">*</span>(<span class="ide">images</span>: <span class="ide">Fun</span>): <span class="ide">Fun</span> <span class="op">=</span></div></div><div class="line"><div class="line-content">  <span class="ide">result</span>[<span class="ide">image</span>, <span class="ide">y</span>, <span class="ide">x</span>, <span class="ide">chan</span>] <span class="op">++=</span> <span class="ide">images</span>[<span class="ide">image</span>, <span class="ide">y</span> <span class="kwd">div</span> <span class="num">2</span>, <span class="ide">x</span> <span class="kwd">div</span> <span class="num">2</span>, <span class="ide">chan</span>] <span class="op">|</span> (<span class="ide">image</span>, <span class="ide">y</span>, <span class="ide">x</span>, <span class="ide">chan</span>)</div></div><div class="line"><div class="line-content">  <span class="ide">result</span><span class="op">.</span><span class="ide">withShape</span>([</div></div><div class="line"><div class="line-content">    <span class="ide">images</span><span class="op">.</span><span class="ide">shape</span>[<span class="num">0</span>],</div></div><div class="line"><div class="line-content">    <span class="ide">images</span><span class="op">.</span><span class="ide">shape</span>[<span class="num">1</span>] <span class="op">*</span> <span class="num">2</span>,</div></div><div class="line"><div class="line-content">    <span class="ide">images</span><span class="op">.</span><span class="ide">shape</span>[<span class="num">2</span>] <span class="op">*</span> <span class="num">2</span>,</div></div><div class="line"><div class="line-content">    <span class="ide">images</span><span class="op">.</span><span class="ide">shape</span>[<span class="num">3</span>]</div></div><div class="line"><div class="line-content">  ])</div></div>  </code></pre>
<p>If the output tensor is not yet declared, the <code>*</code> operator can be added after the tensor&apos;s name to declare it.</p>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="ide">y</span><span class="op">*</span>{<span class="ide">it</span>} <span class="op">++=</span> <span class="ide">input</span>(<span class="str">&quot;x&quot;</span>){<span class="ide">it</span>} <span class="op">*</span> <span class="num">2.0</span> <span class="op">|</span> <span class="ide">it</span></div></div>  </code></pre>
<p>Sometimes you might want to use a custom gradient implementation instead of the one automatically generated by exprgrad.
This is especially useful for ensuring numerical stability or improving performance.
Inside the <code>customGrad</code> attribute, gradient tensors are referred to using the <code>grad(tensor)</code> instruction.</p>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="ide">identity</span><span class="op">*</span>{<span class="ide">x</span>} <span class="op">++=</span> <span class="ide">inp</span>{<span class="ide">x</span>} <span class="op">|</span> <span class="ide">x</span> <span class="kwd">do</span>:</div></div><div class="line"><div class="line-content">  <span class="ide">customGrad</span>:</div></div><div class="line"><div class="line-content">    <span class="ide">grad</span>(<span class="ide">inp</span>){<span class="ide">x</span>} <span class="op">++=</span> <span class="ide">inp</span>{<span class="ide">x</span>} <span class="op">*</span> <span class="ide">grad</span>(<span class="ide">identity</span>){<span class="ide">x</span>} <span class="op">|</span> <span class="ide">x</span></div></div>  </code></pre>
<p>More examples can be found in the <code>exprgrad/layers/base.nim</code> and <code>exprgrad/layers/dnn.nim</code> modules.</p>
<h4>Instructions</h4>
<p>In addition to the basic operators <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>div</code>, <code>mod</code>, <code>==</code>, <code>&lt;</code>, <code>&gt;</code>, <code>&lt;=</code> and <code>&gt;=</code>, the following instructions are supported:</p>
<p>| Instruction          | Description                                                       |
| -------------------- | ----------------------------------------------------------------- |
| <code>sq(x)</code>              | Computes the square of <code>x</code>                                        |
| <code>min(a, b)</code>          | Returns the minimum of <code>a</code> and <code>b</code>                                |
| <code>max(a, b)</code>          | Returns the maximum of <code>a</code> and <code>b</code>                                |
| <code>select(cond, a, b)</code> | Returns <code>a</code> if <code>cond</code> is true else returns <code>b</code>                    |
| <code>sin(x)</code>             | Returns the sine of <code>x</code>                                           |
| <code>cos(x)</code>             | Returns the cosine of <code>x</code>                                         |
| <code>exp(x)</code>             | Computes <code>e ^ x</code>                                                  |
| <code>pow(a, b)</code>          | Computes <code>a ^ b</code>                                                  |
| <code>sqrt(x)</code>            | Computes the square root of <code>x</code>                                   |
| <code>ln(x)</code>              | Computes the natural logarithm of <code>x</code>                             |
| <code>log2(x)</code>            | Computes the logarithm of base 2 of <code>x</code>                           |
| <code>log10(x)</code>           | Computes the logarithm of base 10 of <code>x</code>                          |
| <code>wrap(x, y)</code>         | Computes <code>(x mod y + y) mod y</code> (<code>∈ [0, y) ∩ ℤ</code>)                   |
| <code>toScalar(x)</code>        | Converts <code>x</code> to a <code>Scalar</code> value                                  |
| <code>toIndex(x)</code>         | Converts <code>x</code> to an <code>Index</code> value                                  |
| <code>tensor.shape[dim]</code>  | Returns the size of dimension <code>dim</code> of <code>tensor</code>                   |
| <code>tensor.len</code>         | Returns the number of items in <code>tensor</code>                           |
| <code>tensor.shape.len</code>   | Returns the rank of <code>tensor</code>                                      |
| <code>epoch()</code>            | Returns the current epoch stored in <code>Model.epoch</code>.                |
| <code>arr.len</code>            | Returns the length of the given array.                            |
| <code>arr[index]</code>         | Gets the element stored at <code>index</code> in the array.                  |</p>
<p>If you cannot find the instruction you are looking for, please open an issue.</p>
<h3>Computation Graphs</h3>
<p>Neural networks are represented as computation graphs.
Each computation graph has a set of inputs which are provided to it at run time.
They may be images the model is supposed to classify or a text whose sentiment it is supposed to predict.
Each neural network also has a set of parameters.
These are the internal values which are learned during backpropagation.
Exprgrad refers to the output of a given computation as a target.
A target might be the actual output of the network itself, but also the loss with respect to a training dataset or the action of updating the parameters of the network using gradient descent.
In order to compute the value of a target, a series of kernels (layers) is executed.
Additionally a computation graph may include a set of caches used to save the internal state of an optimizer and randomized tensors used as inputs to dropout layers.</p>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="kwd">proc</span> <span class="ide">param</span><span class="op">*</span>(<span class="ide">shape</span>: <span class="ide">openArray</span>[<span class="ide">int</span>],</div></div><div class="line"><div class="line-content">            <span class="ide">initRange</span>: <span class="ide">HSlice</span>[<span class="ide">float64</span>, <span class="ide">float64</span>] <span class="op">=</span> <span class="op">-</span><span class="num">0.1</span><span class="op">..</span><span class="num">0.1</span>,</div></div><div class="line"><div class="line-content">            <span class="ide">name</span>: <span class="ide">string</span> <span class="op">=</span> <span class="str">&quot;&quot;</span>): <span class="ide">Fun</span></div></div>  </code></pre>
<p>Creates a new parameter with the given shape.
Each parameter is randomly initialized with a uniform distribution in the range <code>initRange</code> after model compilation.</p>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="kwd">proc</span> <span class="ide">input</span><span class="op">*</span>(<span class="ide">name</span>: <span class="ide">string</span>, <span class="ide">shape</span>: <span class="ide">openArray</span>[<span class="ide">int</span>] <span class="op">=</span> []): <span class="ide">Fun</span></div></div>  </code></pre>
<p>Creates a new input with the given name.
The sizes of static dimensions may be specified to enable compiler optimizations.
If a shape is specified unknown dimensions should have the size <code>-1</code>.</p>
<p>Example: <code>input(&quot;x&quot;, [-1, 28, 28, 1])</code></p>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="kwd">proc</span> <span class="ide">target</span><span class="op">*</span>(<span class="ide">fun</span>: <span class="ide">Fun</span>, <span class="ide">name</span>: <span class="ide">string</span>): <span class="ide">Fun</span></div></div>  </code></pre>
<p>Creates a new target with the given name.
Targets may be called using the <code>Model.call</code>, <code>Model.apply</code> or <code>Model.fit</code> procedures.</p>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="kwd">proc</span> <span class="ide">backwards</span><span class="op">*</span>(<span class="ide">fun</span>: <span class="ide">Fun</span>): <span class="ide">Fun</span></div></div>  </code></pre>
<p>Lazily computes the gradients for all parameters of the given computation graph (<code>fun</code>) with respect to the given loss value <code>fun</code>.
Unused gradients are not computed.</p>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="kwd">proc</span> <span class="ide">optimize</span><span class="op">*</span>(<span class="ide">gradients</span>: <span class="ide">Fun</span>,</div></div><div class="line"><div class="line-content">               <span class="ide">params</span>: <span class="ide">HashSet</span>[<span class="ide">Fun</span>],</div></div><div class="line"><div class="line-content">               <span class="ide">optim</span>: <span class="kwd">proc</span> (<span class="ide">param</span>: <span class="kwd">var</span> <span class="ide">Fun</span>, <span class="ide">grad</span>: <span class="ide">Fun</span>)): <span class="ide">Fun</span></div></div><div class="line"><div class="line-content"><span class="kwd">proc</span> <span class="ide">optimize</span><span class="op">*</span>(<span class="ide">gradients</span>: <span class="ide">Fun</span>, <span class="ide">optim</span>: <span class="kwd">proc</span> (<span class="ide">param</span>: <span class="kwd">var</span> <span class="ide">Fun</span>, <span class="ide">grad</span>: <span class="ide">Fun</span>)): <span class="ide">Fun</span></div></div>  </code></pre>
<p>Optimizes the given parameters using the given optimizer.
Optimizers may be created using <code>makeOpt</code>.
The <code>Fun.params</code> procedure may be used to find all parameters of a computation graph.</p>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="kwd">proc</span> <span class="ide">backprop</span><span class="op">*</span>(<span class="ide">loss</span>: <span class="ide">Fun</span>, <span class="ide">optim</span>: <span class="kwd">proc</span> (<span class="ide">param</span>: <span class="kwd">var</span> <span class="ide">Fun</span>, <span class="ide">grad</span>: <span class="ide">Fun</span>)): <span class="ide">Fun</span></div></div>  </code></pre>
<p>Computes the gradients for all parameters of <code>loss</code> and optimizes them using the given optimizer.
Optimizers may be created using <code>makeOpt</code>.
Shortcut for <code>loss.backwards().optimize(optim)</code>.</p>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="kwd">proc</span> <span class="ide">reshape</span><span class="op">*</span>(<span class="ide">fun</span>: <span class="ide">Fun</span>, <span class="ide">shape</span>: <span class="ide">openArray</span>[<span class="ide">int</span>]): <span class="ide">Fun</span></div></div>  </code></pre>
<p>Changes the shape of the given tensor.
Each reshape may include at most one unknown dimension, which should have the value <code>-1</code>.
The length of the tensor must stay constant.</p>
<p>Example: <code>x.reshape([-1, 28 * 28])</code></p>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="kwd">proc</span> <span class="ide">cond</span><span class="op">*</span>(<span class="ide">branches</span>: <span class="ide">openArray</span>[(<span class="ide">string</span>, <span class="ide">Fun</span>)],</div></div><div class="line"><div class="line-content">           <span class="ide">otherwise</span>: <span class="ide">Fun</span> <span class="op">=</span> <span class="kwd">nil</span>): <span class="ide">Fun</span></div></div>  </code></pre>
<p>Selects one of the inputs depending on which target should be evaluated.
Useful for building complex architectures such as GANs.</p>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="kwd">macro</span> <span class="ide">makeOpt</span><span class="op">*</span>(<span class="ide">opt</span>: <span class="ide">typed</span>, <span class="ide">args</span>: <span class="ide">varargs</span>[<span class="ide">untyped</span>]): <span class="ide">untyped</span></div></div>  </code></pre>
<p>Create an optimizer from procedure <code>opt</code> by setting all optional arguments of <code>opt</code>.
The first two arguments to <code>opt</code> are the parameter to optimize and its gradient.
They must have the types <code>var Fun</code> and <code>Fun</code>.
<code>opt</code> may not return a value.</p>
<p>Example: <code>adam.makeOpt(0.01, beta1=0.5)</code></p>
<h3>Models</h3>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="kwd">proc</span> <span class="ide">compile</span><span class="op">*</span>[<span class="ide">T</span>](<span class="ide">graphs</span>: <span class="ide">varargs</span>[<span class="ide">Fun</span>]): <span class="ide">Model</span>[<span class="ide">T</span>]</div></div>  </code></pre>
<p>Compiles a computation graph to a model.
The generic parameter <code>T</code> may be one of <code>float32</code> or <code>float64</code>.</p>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="kwd">proc</span> <span class="ide">call</span><span class="op">*</span>[<span class="ide">T</span>](<span class="ide">model</span>: <span class="ide">Model</span>[<span class="ide">T</span>],</div></div><div class="line"><div class="line-content">              <span class="ide">target</span>: <span class="ide">string</span>,</div></div><div class="line"><div class="line-content">              <span class="ide">args</span>: <span class="ide">openArray</span>[(<span class="ide">string</span>, <span class="ide">Tensor</span>[<span class="ide">T</span>])]): <span class="ide">Tensor</span>[<span class="ide">T</span>]</div></div>  </code></pre>
<p>Computes the value of <code>target</code> for the inputs <code>args</code>.</p>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="kwd">proc</span> <span class="ide">apply</span><span class="op">*</span>[<span class="ide">T</span>](<span class="ide">model</span>: <span class="ide">Model</span>[<span class="ide">T</span>],</div></div><div class="line"><div class="line-content">               <span class="ide">target</span>: <span class="ide">string</span>,</div></div><div class="line"><div class="line-content">               <span class="ide">args</span>: <span class="ide">openArray</span>[(<span class="ide">string</span>, <span class="ide">Tensor</span>[<span class="ide">T</span>])])</div></div>  </code></pre>
<p>Computes <code>target</code> and discards its value.
This procedure is useful for optimizing simple models.
In most cases <code>Model.fit</code> should be preferred since it can train in batches and automatically increments <code>model.epoch</code>.</p>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="kwd">proc</span> <span class="ide">fit</span><span class="op">*</span>[<span class="ide">T</span>](<span class="ide">model</span>: <span class="ide">Model</span>[<span class="ide">T</span>],</div></div><div class="line"><div class="line-content">             <span class="ide">targetName</span>: <span class="ide">string</span>,</div></div><div class="line"><div class="line-content">             <span class="ide">args</span>: <span class="ide">openArray</span>[(<span class="ide">string</span>, <span class="ide">Tensor</span>[<span class="ide">T</span>])],</div></div><div class="line"><div class="line-content">             <span class="ide">batchSize</span>: <span class="ide">int</span> <span class="op">=</span> <span class="num">32</span>,</div></div><div class="line"><div class="line-content">             <span class="ide">logStatus</span>: <span class="ide">bool</span> <span class="op">=</span> <span class="ide">true</span>)</div></div>  </code></pre>
<p>Computes the given target for all batches from the inputs <code>args</code>.
If the sample count is not divisible by the <code>batchSize</code>, the remaining samples are not used in the training process.
This will likely be fixed in the future.</p>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="kwd">proc</span> <span class="ide">emitIr</span><span class="op">*</span>[<span class="ide">T</span>](<span class="ide">model</span>: <span class="ide">Model</span>[<span class="ide">T</span>]): <span class="ide">string</span></div></div>  </code></pre>
<p>Emits intermediate representation for all targets of <code>model</code>.
This is mainly used for debugging purposes.</p>
<h3>IO</h3>
<p>Exprgrad provides an io module which can load commonly used datasets and save/load models to/from disk.</p>
<h4>Saving and Loading Models</h4>
<p>Models can be saved by calling the <code>save</code> procedure from <code>io/serialize</code>.
<code>loadModel</code> is used to load a model from a file.
Since <code>loadModel</code> loads the intermediate representation for the model from the file and compiles it using the JIT compiler, it is <strong>not</strong> recommended to load models from untrusted sources.</p>
<pre>  <code class="language-nim"><div class="line"><div class="line-content"><span class="kwd">let</span> <span class="ide">model</span> <span class="op">=</span> <span class="ide">loadModel</span>[<span class="ide">float32</span>](<span class="str">&quot;model.bin&quot;</span>)</div></div><div class="line"><div class="line-content"><span class="ide">model</span><span class="op">.</span><span class="ide">save</span>(<span class="str">&quot;model.bin&quot;</span>)</div></div>  </code></pre>
<h3>Tensors</h3>
<p>Exprgrad currently uses a simple tensor library providing basic functions aimed at preprocessing datasets for training.
Tensors can be created using <code>Tensor.new</code> and <code>Tensor.rand</code>, printed using <code>$</code> and accessed using the <code>[]</code> and <code>{}</code> operators.
Refer to <code>test/test_tensors.nim</code> for more examples of how to use the tensor library.</p>
<h2>References</h2>
<p>Exprgrad borrows many successful concepts from other projects on array and differentiable programming languages.</p>
<ul>
<li>  <a href="https://halide-lang.org/">Halide</a></li>
<li>  <a href="https://github.com/FluxML/Zygote.jl">Zygote.jl</a></li>
<li>  <a href="https://llvm.org/">LLVM</a></li>
</ul>
<h2>Contributing</h2>
<p>Currently exprgrad is still very early in its development.
All examples shown above already work, but there are still many possibilities for improvement:</p>
<ul>
<li>Improved multithreading</li>
<li>GPU Support</li>
<li>More automatic optimizations (tiling, loop fusion, ...)</li>
<li>...</li>
</ul>
<p>If you would like to contribute to exprgrad, the following tasks might be of interest to you:</p>
<ul>
<li>Integrate with existing tensor libraries</li>
<li>Image loading and saving</li>
<li>Improve batching in <code>fit</code> procedure</li>
<li>Document the tensors module</li>
</ul>
<h3>Project Structure</h3>
<p>The following diagram shows a simplified compilation pipeline which displays the functions of the different modules (files in <code>exprgrad/</code>) of exprgrad&apos;s compiler.</p>
<pre>  <code>          parser       passes       llvmgen
Nim AST –––––––––&gt; IR ––––––––&gt; IR –––––––––&gt; LLVM IR ––&gt; Machine Code 
</code></pre>
<p>Exprgrad&apos;s compiler uses a custom intermediate representation (IR).
All program transformations including the automatic differentiation and optimization are performed within this representation.
It is defined in the module <code>ir.nim</code>.
The current compilation pipeline is defined in the <code>compile</code> procedure of the module <code>model.nim</code>.
All program transformations are currently defined in <code>passes.nim</code>.
Exprgrad uses the LLVM C-API through its own wrapper.
The LLVM IR generator and JIT compiler are defined in <code>llvmgen.nim</code>.</p>
<h2>License</h2>
<p>Copyright 2021 - 2022 Can Joshua Lehmann</p>
<p>Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at</p>
<p>http://www.apache.org/licenses/LICENSE-2.0</p>
<p>Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.</p>
</document>
            
        </div>
        <div class="col-3" id="meta-section">
            <div class="container box rounded p-3">
                
                    <p>
                        <strong>Licence:</strong>
                        Apache License 2.0
                    </p>
                

                
                    <p> <a href="https://github.com/can-lehmann/exprgrad">Project website</a> </p>
                

                
            </div>
        </div>
    </div>
</div>
</div>

<footer class="pt-10 px-3">
  <div class="container pt-4">
    <div class="row mb-4">
      <div class="col-lg-3">
        <h4 class="h5">Getting started with Nim</h4>
        <ul>
          <li><a class="text-decoration-none" href="https://learnxinyminutes.com/docs/nim/">Learn Nim in 5 minutes</a></li>
          <li><a class="text-decoration-none" href="https://nim-by-example.github.io/">Nim by Example</a></li>
          <li><a class="text-decoration-none" href="https://play.nim-lang.org/">Official Playground</a></li>
        </ul>
      </div>
      <div class="col-lg-3">
        <h4 class="h5">Official Tutorials</h4>
        <ul>
          <li><a class="text-decoration-none" href="https://nim-lang.org/docs/tut1.html">General Tutorial</a></li>
          <li><a class="text-decoration-none" href="https://nim-lang.org/docs/tut2.html">Advanced Features</a></li>
          <li><a class="text-decoration-none" href="https://nim-lang.org/docs/tut3.html">Macros and Metaprogramming</a></li>
        </ul>
      </div>
      <div class="col-lg-3">
        <h4 class="h5">Nim for...</h4>
        <ul>
          <li><a class="text-decoration-none" href="https://github.com/nim-lang/Nim/wiki/Nim-for-C-programmers">C programmers</a></li>
          <li><a class="text-decoration-none" href="https://github.com/nim-lang/Nim/wiki/Nim-for-Python-Programmers">Python programmers</a></li>
          <li><a class="text-decoration-none" href="https://github.com/nim-lang/Nim/wiki/Nim-for-TypeScript-Programmers">TypeScript programmers</a></li>
        </ul>
      </div>
      <div class="col-lg-3">
        <h4 class="h5">Documentation</h4>
        <ul>
          <li><a class="text-decoration-none" href="https://nim-lang.org/docs/lib.html">Standard Library</a></li>
          <li><a class="text-decoration-none" href="https://nim-lang.org/docs/manual.html">Language Manual</a></li>
          <li><a class="text-decoration-none" href="https://nim-lang.org/docs/tools.html">Tools Documentation</a></li>
        </ul>
      </div>
    </div>
    <div class="row mt-3">
      <div class="col-12 text-center mt-4 mx-auto">
        <a href="#" class="d-block mb-3 mx-auto bw"></a>
        <p class="text-muted">
        Created in <a href="https://nim-lang.org/">Nim</a> on <a href="https://pages.github.com/">GitHub Pages</a>.
        <a href="https://github.com/gabbhack/nimidirectory">Available</a>
        under <a href="https://en.wikipedia.org/wiki/GNU_General_Public_License#Version_3">GPLv3</a>
        </p>
        <p class="text-muted">
        Builded at 2023-11-27T01:11:57Z
        </p>
      </div>
    </div>
  </div>
</footer>
<script>
</script>
</body>
</html>
